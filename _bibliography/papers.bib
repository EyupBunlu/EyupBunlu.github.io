---
---


@Article{qc1,
AUTHOR = {Cara, Mar\c{c}al and others},
TITLE = {Quantum Vision Transformers for q-g Classification},
JOURNAL = {Axioms},
VOLUME = {13},
YEAR = {2024},
NUMBER = {5},
ARTICLE-NUMBER = {323},
URL = {https://www.mdpi.com/2075-1680/13/5/323},
ISSN = {2075-1680},
ABSTRACT = {We introduce a hybrid quantum-classical vision transformer architecture, notable for its integration of variational quantum circuits within both the attention mechanism and the multi-layer perceptrons. The research addresses the critical challenge of computational efficiency and resource constraints in analyzing data from the upcoming High Luminosity Large Hadron Collider, presenting the architecture as a potential solution. In particular, we evaluate our method by applying the model to multi-detector jet images from CMS Open Data. The goal is to distinguish quark-initiated from gluon-initiated jets. We successfully train the quantum model and evaluate it via numerical simulations. Using this approach, we achieve classification performance almost on par with the one obtained with the completely classical architecture, considering a similar number of parameters.},
DOI = {10.3390/axioms13050323}
}

@Article{qc2,
AUTHOR = {Dong, Zhongtian and Comajoan Cara, Marçal and Dahale, Gopal Ramesh and Forestano, Roy T. and Gleyzer, Sergei and Justice, Daniel and Kong, Kyoungchul and Magorsch, Tom and Matchev, Konstantin T. and Matcheva, Katia and Unlu, Eyup B.},
TITLE = {ℤ2 × ℤ2 Equivariant Quantum Neural Networks: Benchmarking against Classical Neural Networks},
JOURNAL = {Axioms},
VOLUME = {13},
YEAR = {2024},
NUMBER = {3},
ARTICLE-NUMBER = {188},
URL = {https://www.mdpi.com/2075-1680/13/3/188},
ISSN = {2075-1680},
ABSTRACT = {This paper presents a comparative analysis of the performance of Equivariant Quantum Neural Networks (EQNNs) and Quantum Neural Networks (QNNs), juxtaposed against their classical counterparts: Equivariant Neural Networks (ENNs) and Deep Neural Networks (DNNs). We evaluate the performance of each network with three two-dimensional toy examples for a binary classification task, focusing on model complexity (measured by the number of parameters) and the size of the training dataset. Our results show that the Z2×Z2 EQNN and the QNN provide superior performance for smaller parameter sets and modest training data samples.},
DOI = {10.3390/axioms13030188}
}




@Article{qc3,
AUTHOR = {Unlu, Eyup B. and Comajoan Cara, Marçal and Dahale, Gopal Ramesh and Dong, Zhongtian and Forestano, Roy T. and Gleyzer, Sergei and Justice, Daniel and Kong, Kyoungchul and Magorsch, Tom and Matchev, Konstantin T. and Matcheva, Katia},
TITLE = {Hybrid Quantum Vision Transformers for Event Classification in High Energy Physics},
JOURNAL = {Axioms},
VOLUME = {13},
YEAR = {2024},
NUMBER = {3},
ARTICLE-NUMBER = {187},
URL = {https://www.mdpi.com/2075-1680/13/3/187},
ISSN = {2075-1680},
ABSTRACT = {Models based on vision transformer architectures are considered state-of-the-art when it comes to image classification tasks. However, they require extensive computational resources both for training and deployment. The problem is exacerbated as the amount and complexity of the data increases. Quantum-based vision transformer models could potentially alleviate this issue by reducing the training and operating time while maintaining the same predictive power. Although current quantum computers are not yet able to perform high-dimensional tasks, they do offer one of the most efficient solutions for the future. In this work, we construct several variations of a quantum hybrid vision transformer for a classification problem in high-energy physics (distinguishing photons and electrons in the electromagnetic calorimeter). We test them against classical vision transformer architectures. Our findings indicate that the hybrid models can achieve comparable performance to their classical analogs with a similar number of parameters.},
DOI = {10.3390/axioms13030187},
selected = {true}
}



@Article{qc4,
AUTHOR = {Forestano, Roy T. and Comajoan Cara, Marçal and Dahale, Gopal Ramesh and Dong, Zhongtian and Gleyzer, Sergei and Justice, Daniel and Kong, Kyoungchul and Magorsch, Tom and Matchev, Konstantin T. and Matcheva, Katia and Unlu, Eyup B.},
TITLE = {A Comparison between Invariant and Equivariant Classical and Quantum Graph Neural Networks},
JOURNAL = {Axioms},
VOLUME = {13},
YEAR = {2024},
NUMBER = {3},
ARTICLE-NUMBER = {160},
URL = {https://www.mdpi.com/2075-1680/13/3/160},
ISSN = {2075-1680},
ABSTRACT = {Machine learning algorithms are heavily relied on to understand the vast amounts of data from high-energy particle collisions at the CERN Large Hadron Collider (LHC). The data from such collision events can naturally be represented with graph structures. Therefore, deep geometric methods, such as graph neural networks (GNNs), have been leveraged for various data analysis tasks in high-energy physics. One typical task is jet tagging, where jets are viewed as point clouds with distinct features and edge connections between their constituent particles. The increasing size and complexity of the LHC particle datasets, as well as the computational models used for their analysis, have greatly motivated the development of alternative fast and efficient computational paradigms such as quantum computation. In addition, to enhance the validity and robustness of deep networks, we can leverage the fundamental symmetries present in the data through the use of invariant inputs and equivariant layers. In this paper, we provide a fair and comprehensive comparison of classical graph neural networks (GNNs) and equivariant graph neural networks (EGNNs) and their quantum counterparts: quantum graph neural networks (QGNNs) and equivariant quantum graph neural networks (EQGNN). The four architectures were benchmarked on a binary classification task to classify the parton-level particle initiating the jet. Based on their area under the curve (AUC) scores, the quantum networks were found to outperform the classical networks. However, seeing the computational advantage of quantum networks in practice may have to wait for the further development of quantum technology and its associated application programming interfaces (APIs).},
DOI = {10.3390/axioms13030160}
}


@article{sym1,
  title = {Deep learning symmetries and their Lie groups,  algebras,  and subalgebras from first principles},
  volume = {4},
  ISSN = {2632-2153},
  url = {http://dx.doi.org/10.1088/2632-2153/acd989},
  DOI = {10.1088/2632-2153/acd989},
  number = {2},
  journal = {Machine Learning: Science and Technology},
  publisher = {IOP Publishing},
  author = {Forestano,  Roy T and Matchev,  Konstantin T and Matcheva,  Katia and Roman,  Alexander and Unlu,  Eyup B and Verner,  Sarunas},
  year = {2023},
  month = jun,
  pages = {025027}
}

@article{sym2,
  title = {Oracle-Preserving Latent Flows},
  volume = {15},
  ISSN = {2073-8994},
  url = {http://dx.doi.org/10.3390/sym15071352},
  DOI = {10.3390/sym15071352},
  number = {7},
  journal = {Symmetry},
  publisher = {MDPI AG},
  author = {Roman,  Alexander and Forestano,  Roy T. and Matchev,  Konstantin T. and Matcheva,  Katia and Unlu,  Eyup B.},
  year = {2023},
  month = jul,
  pages = {1352},
  selected = {true}
}

@article{sym3,
  title = {Discovering sparse representations of Lie groups with machine learning},
  volume = {844},
  ISSN = {0370-2693},
  url = {http://dx.doi.org/10.1016/j.physletb.2023.138086},
  DOI = {10.1016/j.physletb.2023.138086},
  journal = {Physics Letters B},
  publisher = {Elsevier BV},
  author = {Forestano,  Roy T. and Matchev,  Konstantin T. and Matcheva,  Katia and Roman,  Alexander and Unlu,  Eyup B. and Verner,  Sarunas},
  year = {2023},
  month = sep,
  pages = {138086}
}
@article{sym4,
  title = {Accelerated discovery of machine-learned symmetries: Deriving the exceptional Lie groups G2,  F4 and E6},
  volume = {847},
  ISSN = {0370-2693},
  url = {http://dx.doi.org/10.1016/j.physletb.2023.138266},
  DOI = {10.1016/j.physletb.2023.138266},
  journal = {Physics Letters B},
  publisher = {Elsevier BV},
  author = {Forestano,  Roy T. and Matchev,  Konstantin T. and Matcheva,  Katia and Roman,  Alexander and Unlu,  Eyup B. and Verner,  Sarunas},
  year = {2023},
  month = dec,
  pages = {138266}
}

@article{sym5,
title = {Identifying the group-theoretic structure of machine-learned symmetries},
journal = {Physics Letters B},
volume = {847},
pages = {138306},
year = {2023},
issn = {0370-2693},
doi = {https://doi.org/10.1016/j.physletb.2023.138306},
url = {https://www.sciencedirect.com/science/article/pii/S0370269323006408},
author = {Roy T. Forestano and Konstantin T. Matchev and Katia Matcheva and Alexander Roman and Eyup B. Unlu and Sarunas Verner},
abstract = {Deep learning was recently successfully used in deriving symmetry transformations that preserve important physics quantities. Being completely agnostic, these techniques postpone the identification of the discovered symmetries to a later stage. In this letter we propose methods for examining and identifying the group-theoretic structure of such machine-learned symmetries. We design loss functions which probe the subalgebra structure either during the deep learning stage of symmetry discovery or in a subsequent post-processing stage. We illustrate the new methods with examples from the U(n) Lie group family, obtaining the respective subalgebra decompositions. As an application to particle physics, we demonstrate the identification of the residual symmetries after the spontaneous breaking of non-Abelian gauge symmetries like SU(3) and SU(5) which are commonly used in model building.},
selected={true}
}

@InProceedings{ariel1,
author={ Unlu, Eyup B.
and Forestano, Roy T.
and Matchev, Konstantin T.
and Matcheva, Katia},
editor={Meo, Rosa
and Silvestri, Fabrizio},
title={Reproducing Bayesian Posterior Distributions for Exoplanet Atmospheric Parameter Retrievals with a Machine Learning Surrogate Model",
booktitle="Machine Learning and Principles and Practice of Knowledge Discovery in Databases},
year={2025},
publisher={Springer Nature Switzerland},
address="Cham",
pages={100--112},
abstract={We describe a machine-learning-based surrogate model for reproducing the Bayesian posterior distributions for exoplanet atmospheric parameters derived from transmission spectra of transiting planets with typical retrieval software such as TauRex. The model is trained on ground truth distributions for seven parameters: the planet radius, the atmospheric temperature, and the mixing ratios for five common absorbers: {\$}{\$}H{\_}2O{\$}{\$}H2O, {\$}{\$}CH{\_}4{\$}{\$}CH4, {\$}{\$}NH{\_}3{\$}{\$}NH3, CO and {\$}{\$}CO{\_}2{\$}{\$}CO2. The model performance is enhanced by domain-inspired preprocessing of the features and the use of semi-supervised learning in order to leverage the large amount of unlabelled training data available. The model was among the winning solutions in the 2023 Ariel Machine Learning Data Challenge.},
isbn={978-3-031-74643-7},
selected = {true}
}



@InProceedings{ariel2,
  title = 	 {Lessons Learned from Ariel Data Challenge 2022 - Inferring Physical Properties of Exoplanets From Next-Generation Telescopes},
  author =       {Yip, Kai Hou and Changeat, Quentin and Waldmann, Ingo and Unlu, Eyup B. and Forestano, Roy T. and Roman, Alexander and Matcheva, Katia and Matchev, Konstantin T. and Stefanov, Stefan and Podsztavek, Ond\vrej and Morvan, Mario and Nikolaou, Nikolaos and Al-Refaie, Ahmed and Jenner, Clare and Johnson, Chris and Tsiaras, Angelos and Edwards, Billy and Alves de Oliveira, Catarina and Thiyagalingam, Jeyan and Lagage, Pierre-Olivier and Cho, James and Tinetti, Giovanna},
  booktitle = 	 {Proceedings of the NeurIPS 2022 Competitions Track},
  pages = 	 {1--17},
  year = 	 {2022},
  editor = 	 {Ciccone, Marco and Stolovitzky, Gustavo and Albrecht, Jacob},
  volume = 	 {220},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28 Nov--09 Dec},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v220/yip23a/yip23a.pdf},
  url = 	 {https://proceedings.mlr.press/v220/yip23a.html},
  abstract = 	 {Exo-atmospheric studies, i.e. the study of exoplanetary atmospheres, is an emerging frontier in Planetary Science. To understand the physical properties of hundreds of exoplanets, astronomers have traditionally relied on sampling-based methods. However, with the growing number of exoplanet detections (i.e. increased data quantity) and advancements in technology from telescopes such as JWST and Ariel (i.e. improved data quality), there is a need for more scalable data analysis techniques. The Ariel Data Challenge 2022 aims to find interdisciplinary solutions from the NeurIPS community. Results from the challenge indicate that machine learning (ML) models have the potential to provide quick insights for thousands of planets and millions of atmospheric models. However, the machine learning models are not immune to data drifts, and future research should investigate ways to quantify and mitigate their negative impact.},
  selected = {true}
}


@article{ariel3,
doi = {10.3847/1538-4357/ad0047},
url = {https://dx.doi.org/10.3847/1538-4357/ad0047},
year = {2023},
month = {nov},
publisher = {The American Astronomical Society},
volume = {958},
number = {2},
pages = {106},
author = {Roy T. Forestano and Konstantin T. Matchev and Katia Matcheva and Eyup B. Unlu},
title = {Searching for Novel Chemistry in Exoplanetary Atmospheres Using Machine Learning for Anomaly Detection},
journal = {The Astrophysical Journal},
abstract = {The next generation of telescopes will yield a substantial increase in the availability of high-quality spectroscopic data for thousands of exoplanets. The sheer volume of data and number of planets to be analyzed greatly motivate the development of new, fast, and efficient methods for flagging interesting planets for reobservation and detailed analysis. We advocate the application of machine learning (ML) techniques for anomaly (novelty) detection to exoplanet transit spectra, with the goal of identifying planets with unusual chemical composition and even searching for unknown biosignatures. We successfully demonstrate the feasibility of two popular anomaly detection methods (local outlier factor and one-class support vector machine) on a large public database of synthetic spectra. We consider several test cases, each with different levels of instrumental noise. In each case, we use receiver operating characteristic curves to quantify and compare the performance of the two ML techniques.}
}
